{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.1 특징(feature)란?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 특징\n",
    "\n",
    "모델은 원하는 출력 결과를 도출하는 입력 샘플을 다른 샘플들과 구분해낼 수 있는 능력이 필요하다. 따라서 데이터의 샘플을 잘 나타내는 특징을 추출하고 학습할 수 있어야 한다.  \n",
    "MNIST 데이터셋 예제에서는 데이터 샘플의 숫자를 분류하기 위해서 다음과 같은 정보들이 필요할 것 이다.\n",
    "* 곧은 선과 휘어진 선이 얼마나 있는가?\n",
    "* 곧은 선과 휘어진 선들이 서로 어떻게 이어져 있는가?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 특징 추출 방법: 머신러닝 vs 딥러닝\n",
    "\n",
    "전통적인 방식의 머신러닝에서는 주로 사람이 직접 가정을 세우고 특징을 추출하는 방법을 설정한다.  \n",
    "이를 위해서 데이터를 면밀하게 분석하는 과정을 거쳐야 하고, 어떠한 가정에 따라 전처리를 수행하여 특징을 추출할 수 있다.  \n",
    "장점으로는 모델의 동작 및 결과 해석을 하기 쉽다. 단점으로는 가설의 설정이 잘못되거나 미처 생각하지 못한 특징이 존재할 수 있다는 리스크도 존재한다.\n",
    "\n",
    "딥러닝을 활용한 학습 수행의 경우에는 데이터를 날(raw)것의 상태로 넣어준다. 그리고 신경망 모델이 직접 특징을 파악하고 추출하는 과정을 거쳐 분류 또는 회귀 작업을 수행한다.  \n",
    "장점으로는 구현이 용이하며 사람이 발견할 수 없는 특징들도 활용할 수 있다는 장점이 있다. 딥러닝이 뛰어난 성능을 보여주는 이유 중 하나일 것 이다.  \n",
    "단점으로는 딥러닝 모델은 내부적으로 스스로 추출하고 가공한 특징을 활용하기 때문에 이를 해석하기 어렵다는 것이다. 따라서 모델의 성능이 만족스럽지 않을 때는 원인을 분석하는 데 어려움을 겪을 가능성이 높다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 특징 벡터\n",
    "\n",
    "특징 벡터(feature vector)는 앞서 살펴보았던 특징들을 모아서 벡터로 표현한 것 이다. 엑셀 파일과 같은 테이블로 구성된 데이터셋의 각 행(row)도 이에 해당할 수 있다.  \n",
    "각 열(column)은 벡터가 되면 차원을 이루고 벡터의 각 차원은 어떤 속성에 대한 수치를 나타낼 수 있다.  \n",
    "수치가 비슷할수록 비슷한 샘플이라고 생각할 수 있을 것이다. 즉, 특징 벡터를 구성하면 이를 통해 샘플 사이의 거리(유사도)를 계산할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.2 원 핫 인코딩"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 연속 vs 카테고리 값\n",
    "\n",
    "우리가 다루는 데이터 샘플은 보통 연속 값과 카테고리 값 두 가지로 구성된다.  \n",
    "연속 값은 키, 몸무게 와 같은 실수로 표현될 수 있는 값이다. 카테고리 값은 보통 이산 값이며 단어나 클래스로 표현된다.  \n",
    "연속 값은 비슷한 값이라면 서로 비슷하다는 의미를 지니지만, 카테고리 값은 비슷한 값일지라도 서로 상관 없다는 의미를 지닌다.  \n",
    "따라서 우리는 카테고리 값을 특징 벡터로 표현할 때에 인덱스 값으로 표기하는 대신 다른 방법을 선택해야 한다. 코사인 유사도나 유클리디안 거리에서 이상한 계산이 이루어지기 때문."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 원 핫 인코딩\n",
    "\n",
    "원 핫 인코딩이란 크기가 의미를 갖는 정수로 나타내는 대신에 한개의 1과 n-1개의 0으로 나누어진 n차원의 벡터로 나타내는 방법을 의미한다.  \n",
    "원 핫 벡터들은 서로 직교하고, 서로 다른 두 벡터의 코사인 유사도는 항상 0이며 유클라디안 거리는 루트2 이다.  \n",
    "\n",
    "이처럼 벡터 대부분의 차원이 0인 경우를 희소 벡터라고 부른다. 이와 반대되는 개념은 고밀도 벡터이다.  \n",
    "앞서 본것처럼 희소 벡터의 경우에는 유사도 계산이나 거리 계산을 통해 샘플 사이의 관게를 파악하는 데 어려움을 겪을 수 있다. 원 핫 벡터는 희소 벡터의 장점이라고 할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 단어 임베딩\n",
    "\n",
    "자연어 처리는 단어를 데이터로 다루는 대표적인 분야이다. 따라서 단어를 모델에 입력으로 넣어주기 위해서 어쩔 수 없이 원 핫 인코딩 벡터를 활용해야 한다.  \n",
    "하지만 단어의 개수는 매우 많기 때문에 차원이 커져서 비효율 적이며, 단어 사이의 유사도를 표현할 수 없다.  \n",
    "\n",
    "이때 필요한 것이 단어 임베딩이다. 단어 임베딩 기법을 통해 원 핫 벡터로 표현된 단어를 고밀도 벡터로 표현 할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.3 차원 축소\n",
    "\n",
    "굳이 높은 차원의 공간에서 데이터를 표현할 필요가 없다. 따라서 차원 축소를 진행한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 선형 차원 축소: 주성분분석[PCA]\n",
    "PCA는 가장 널리 사용되는 차원 축소 방법 중의 하나로, 고차원의 공간에 샘플들이 분포 하고 있을 때, 분포를 잘 설명하는 새로눈 축(axis)를 찾아내는 과정을 말한다.  \n",
    "차원 축소를 수행하면 앞과 같이 희소한 벡터들이 존재할 때 좀더 밀도 높은 표현의 벡터로 나타낼 수 있게 된다.\n",
    "\n",
    "<img src = \"표현학습1.jpg\" width = \"400\" height = \"400\">\n",
    "\n",
    "다음의 그림과 같이, 주어진 데이터 샘플들에 대해 두 가지 조건을 만족하는 축을 찾아 해당 축에 샘플을 투사 하면 낮은 차원의 샘플로 변환이 가능하다.\n",
    "\n",
    "<img src = \"표현학습2.jpg\" width = \"400\" height = \"300\">\n",
    "\n",
    "이때, 새롭게 찾아낸 축에 투사된 샘플들의 분산이 클수록 샘플들 사이의 특징을 잘 표현할 수 있으며, 투사하는 거리가 작아질수록 잃어버리는 정보가 줄어들게 된다.  \n",
    "PCA를 통해 성공적인 차원 축소를 수행하면 두 그룹을 분리하는 경계선도 성공적으로 찾아낼 수 있다.\n",
    "\n",
    "<img src = \"표현학습3.jpg\" width = \"400\" height = \"300\">\n",
    "\n",
    "하지만 PCA는 선형적으로 차원 축소를 수행하기 때문에 비선형적인 경계선을 갖는 경우에는 PCA를 통해 분류를 잘 수행할 수 없다.  \n",
    "이러한 케이스를 해결하기 위해서 선형 차원 축소를 뛰어넘어 비선형 차원 축소를 도입해야한다.\n",
    "\n",
    "<img src = \"표현학습4.jpg\" width = \"400\" height = \"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.4 오토인코더\n",
    "\n",
    "비선형 차원 축소를 수행할 수 있는 방법 중 하나는 오토인코더가 있다. 오토인코더는 비선형 차원 축소를 수행할 수 있도록 하는 심층 신경망이다.\n",
    "\n",
    "<img src = \"표현학습5.jpg\" width = \"400\" height = \"200\">\n",
    "\n",
    "오토인코더는 인코더와 디코더로 구성되어 있으며, 인코드를 통해 데이터의 차원 축소와 복원을 수행한다. 이 과정은 마치 압축과 해제와 같다.  \n",
    "인코더는 최대한 입력 샘플의 정보를 보존하도록 손실 압축을 진행하고, 디코더는 인코더의 중간 결과물을 받아 입력 샘플과 같아지도록 압축 해제를 수행한다.  \n",
    "복원을 성공적으로 진행하기 위해서 오토인코더는 학습 데이터의 특징을 추출하는 방법을 자동으로 학습하게 된다.  \n",
    "참고로 오토인코더는 입력 샘플 x와 출력 벡터 x^의 MSE 손실을 최소화 하도록 학습한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 인코더와 디코더의 역할\n",
    "\n",
    "인코더는 고차원의 입력을 받아 복원에 필요한 정보를 중심으로 손실 압축을 진행한다. 그러므로 인코더가 뱉어낸 중간 결과물 z는 입력 샘플보다 낮은 차원의 벡터이다.  \n",
    "인코더는 효율적인 압축을 수행하기 위해서 중요한 특징이 무엇인지 자동으로 파악하고 뻔한 정보는 버릴 것 이다.  \n",
    "인코더와 디코더 사이의 병목(bottleneck)구간에 존재하는 인코더 중간 결과물 z는 입력 샘플을 최대한 보존하고 있다. 따라서 디코더는 주어진 중간 결과물 벡터를 활용하여 입력 샘플을 최대한 복원할 수 있을 것이다.  \n",
    "이 과정에서 중간 결과물 벡터는 효율적인 압축을 수행해야 하므로 자연스럽게 고밀도 벡터로 표현된다. 또한 중간 결과물 벡터 또한 입력 샘플에 대한 특징 벡터라고 볼 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 은닉 표현\n",
    "\n",
    "중간 결과물 벡터 z를 입력 샘플 x에 대한 은닉 표현이라고 부른다. 그리고 해당 벡터가 존재하는 공간을 은닉 공간(hidden space)라고 부른다.  다음 그림은 MNISST 데이터셋을 은닉 공간에 표현한 것 이다.  \n",
    "여기서 각각의 색깔은 특정 숫자를 의미한다.\n",
    "\n",
    "<img src = \"표현학습6.jpg\" width = \"400\" height = \"400\"> \n",
    "\n",
    "은닉 벡터는 원하는 출력을 뱉어내는데 필요한 정보를 해당 모델만 알고 있는 방법으로 감춰놓고 있다.  \n",
    "이것은 꼭 오토인코더와 같이 입력 샘플을 똑같이 복원하는 데에만 적용되는 것이 아니라 분류와 같은 문제에도 적용된다.  \n",
    "분류를 위한 정보는 샘플을 똑같이 복원을 위한 정보보다 훨씬 적을 것이다. 그리고 해당 중간 결과물 벡터는 아마도 입력 샘플보다 훨씬 작은 차원으로 표현되고 있을 것이다.  \n",
    "따라서 해당 중간 결과물 벡터도 당연히 입력 샘플에 대한 특징 벡터라고 볼 수 있으며, 은닉 벡터라고도 할 수 있다.\n",
    "\n",
    "<img src = \"표현학습7.jpg\" width = \"400\" height = \"200\">\n",
    "\n",
    "위 그림에선 3개의 게층이 존재하고, 2개의 은닉 표현 h1, h2가 존재하는 것을 볼 수 있다. 또한 입력 벡터 x의 차원 크기에 비해 은닉 표현이 출력에 가까워질수록 더 작아지는 것을 볼 수 있다.  \n",
    "보통은 이처럼 입력에 비해 더 작거나 비슷한 차원을 활용하여 데이터의 비선형 관계를 풀어내도록 모델의 구조를 설계한다.  \n",
    "따라서 오토인코더의 병목 구간처럼 x -> y 관계를 예측하는 데 필요한 정보를 추출할 수 있도록 신경망의 중간 계층들이 학습될 것 이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.5 마치며\n",
    "\n",
    "### 특징\n",
    "\n",
    "* 좋은 특징이란 모델이 우너하는 출력 결과를 뱉어내기 위해 해당 출력을 지니는 입력 샘플들을 다른 샘플들로부터 구분해낼 수 있는 특징을 말함  \n",
    "* 모델은 데이터의 샘플을 잘 설명하는 특징을 추출하고 학습할 수 있어야 함\n",
    "* 기존 머신러닝은 사용자가 가정을 세우고 직접 뽑아낸 특징 hand-crafted feature을 활용하여 모델이 패턴을 인식\n",
    "* 딥러닝은 날(raw)것의 데이터에서 모델이 직접 특징을 추출하여 학습을 수행\n",
    "\n",
    "### 원 핫 인코딩\n",
    "\n",
    "* 연속 값과 카테고리 값의 결정적인 차이점으로 연속 값은 비슷한 값이라면 서로 비슷하다는 의미를 지니지만 카테고리 값은 비슷한 값일지라도 서로 상관 없다는 의미를 가짐\n",
    "* 원 핫 인코딩은 카테고리 값을 표현하기 위한 방법\n",
    "* 크기가 의미를 갖는 정수로 나타내는 방법 대신, 한 개의 1과 n-1개의 0으로 이루어진 n차원의 벡터를 통해 표현\n",
    "\n",
    "### 은닉 표현\n",
    "\n",
    "* 잠재 표현이라고 부르기도 함\n",
    "* 심층신경망 내부 계층이 데이터를 잘 설명할 수 있도록 출력 차원인 저차원의 공간으로 비선형 변환을 하는 과정에서 얻어지는 결과물\n",
    "* 비슷한 성격의 샘플들은 비슷한 은닉 표현을 가지고 있을 가능성이 높음\n",
    "* 은닉 표현이 어떠한 의미를 지니는지 해석하는 것은 매우 어려움"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
