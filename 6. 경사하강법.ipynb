{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd15e543",
   "metadata": {},
   "source": [
    "# 6.1 미분이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b62b1",
   "metadata": {},
   "source": [
    "### 1. 기울기\n",
    "\n",
    "기울기는 x 증가량에 대한 y 증가량으로 정의 된다. 수식은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfd80c",
   "metadata": {},
   "source": [
    "<img src = \"기울기1.jpg\" width = \"300\" height = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa9e6c",
   "metadata": {},
   "source": [
    "### 2. 극한(무한소)과 미분\n",
    "\n",
    "x의 변화량을 매우 작게 만들어 h = x2-x1 에서 h를 0에 가깝게 만들어본다고 가정하면, \n",
    "\n",
    "<img src = \"극한과미분1.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "함수 f 위의 지점 (x,f(x))에서의 접선의 기울기 라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a8d89",
   "metadata": {},
   "source": [
    "### 3. 도함수\n",
    "\n",
    "함수 f가 주어졌을 때, 특정 지점 x에 대한 접선의 기울기를 함수로 나타낼 수 있다.\n",
    "\n",
    "<img src = \"도함수2.jpg\" width = \"300\" height = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c505f",
   "metadata": {},
   "source": [
    "### 4. 뉴턴 vs 라이프니츠\n",
    "\n",
    "뉴턴 표기법 : y' = f'(x)\n",
    "라이프니츠 표기법 : dy/dx = df/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a515a7f",
   "metadata": {},
   "source": [
    "### 5. 합성함수 미분\n",
    "\n",
    "<img src = \"합성함수미분1.jpg\" width = \"300\" height = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc8178",
   "metadata": {},
   "source": [
    "<img src = \"합성함수미분2.jpg\" width = \"300\" height = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e96eb",
   "metadata": {},
   "source": [
    "# 6.2 편미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2ba7c",
   "metadata": {},
   "source": [
    "### 1. 편미분\n",
    "\n",
    "입력 변수가 x,y로 정의되는 함수가 있다고 해보자. z = f(x,y)  \n",
    "이때, 하나의 입력 변수에 대해서만 미분을 수행할 수 있다. 이것을 편미분이라고 한다. 수식은 다음과 같다.  \n",
    "\n",
    "<img src = \"편미분1.jpg\" width = \"300\" height = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370aeb2c",
   "metadata": {},
   "source": [
    "각 미분 결과는 기울기의 크기를 나타낸다. 다변수 함수 f에서 특정지점 (x,y)의 기울기는 각 변수별 미분 결과를 합쳐서 벡터로 나타내게 된다.\n",
    "\n",
    "<img src = \"편미분2.jpg\" width = \"300\" height = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c4e59",
   "metadata": {},
   "source": [
    "이 기울기 벡터를 그래디언트(gradient)라고 부른다.  \n",
    "다차원 함수로 정의된 평면에서의 접면의 벡터라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f832f",
   "metadata": {},
   "source": [
    "### 2. 함수의 입출력 형태\n",
    "\n",
    "벡터를 입력으로 받는 함수  \n",
    "<img src = \"함수의입출력형태1.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "행렬을 입력으로 받는 함수  \n",
    "<img src = \"함수의입출력형태2.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "출력이 벡터 또는 행렬인 함수\n",
    "<img src = \"함수의입출력형태3.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "입력과 출력이 벡터인 함수\n",
    "<img src = \"함수의입출력형태4.jpg\" width = \"300\" height = \"300\">  \n",
    "n개의 입력을 받아 m개의 출력을 내보내는 선형 계층도 여기에 속합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ff3cb",
   "metadata": {},
   "source": [
    "### 3. 행렬의 미분\n",
    "\n",
    "스칼라를 벡터로 미분  \n",
    "<img src = \"행렬의미분1.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "스칼라를 행렬로 미분\n",
    "<img src = \"행렬의미분2.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "벡터를 스칼라로 미분  \n",
    "<img src = \"행렬의미분3.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "벡터를 벡터로 미분\n",
    "<img src = \"행렬의미분4.jpg\" width = \"300\" height = \"300\">\n",
    "각 m개의 함수를 벡터 x의 각 n개의 요소로 미분할 경우에 nxm의 행렬 형태로 미분 결과가 나올 것 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9237c2",
   "metadata": {},
   "source": [
    "# 6.3 경사하강법\n",
    "\n",
    "알 수 없는 함수 f를 근사계산 하기 위해서 데이터셋 D를 모아 모델 함수가 최적의 출력값을 반환하는(손실함수가 최소가 되는) 모델의 가중치 파라미터 theta^를 찾고 싶다.  \n",
    "이때 선형계층만을 모델로 활용할 때 수식으로 나타내면 다음과 같다.  \n",
    "<img src = \"경사하강법1.jpg\" width = \"300\" height = \"300\">  \n",
    "해당 파라미터를 찾기 위해 가중치 파라미터의 값을 랜덤하게 생성할 순 있으나, 비효율적이다.  \n",
    "이 때 효율적으로 손실 함수의 출력을 최소로 만드는 입력을 찾기위해 \"경사하강법\"을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4bdf7",
   "metadata": {},
   "source": [
    "### 1. 1차원에서의 경사하강법\n",
    "\n",
    "경사하강법은 미분 가능한 복잡한 함수가 있을 때 해당 함수의 최소점을 찾기위한 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533dceb",
   "metadata": {},
   "source": [
    "### 2. 경사하강법의 수식 표현\n",
    "\n",
    "수식에서 함수 f의 출력 y는 공의 세로축 위치인 높이를 의미한다.  \n",
    "<img src = \"경사하강법의수식표현1.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "따라서 함수 f의 출력값 y를 미분하면 각 지점에 대한 기울기를 구알 수 있다.  \n",
    "그다음 움직여야 하는 다음 지점의 위치는 현재 지점의 기울기를 빼면 구할 수 있다. 이 때, 기울기에 상수 n을 곱해주어 움직임의 속도를 제어할 수 있다.  \n",
    "상수 n을 학습률이라고 부른다.  \n",
    "이런 식으로 공의 위치를 계속해서 바꿔주다 보면 언젠가는 공이 더 이상 움직일 수 없는 곳에 도달한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42bd96f",
   "metadata": {},
   "source": [
    "공의 위치 x에 대응되는 가중치 파라미터 theta는 손실 함수의 출력값을 낮추기 위해서 변경(업데이트) 되어야 한다.  \n",
    "마찬가지로 손실 함수의 출력값L(theta)은 공의 높이 y에 대응된다.  \n",
    "손실 함수를 가중치 파라미터로 미분하여 얻은 그래디언트(상수 n을 곱한 후) 현재 가중치 파라미터에서 빼준다.  \n",
    "<img src = \"경사하강법의수식표현2.jpg\" width = \"300\" height = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df4487",
   "metadata": {},
   "source": [
    "### 3. 전역 최소점과 지역 최소점\n",
    "\n",
    "<img src = \"전역최소점1.jpg\" width = \"400\" height = \"400\">\n",
    "\n",
    "우리가 목표로 하는 지점은 전체 구간에서 가장 낮은 함수의 출력값을 갖는 지점이 된다. 손실 함수를 대입하면 손실 값이 최소가 되는 모델의 가중치 파라미터가 될 것 이다.  \n",
    "우리는 이 지점을 \"전역 최소점\"이라고 부른다.  \n",
    "반대로 녹색 화살표들이 가리키고 있는 지점을 \"지역 최소점\"이라고 부른다.\n",
    "\n",
    "경사하강법은 미분 결과를 활용하므로 지역 최소점에 빠질 우려가 있다. 지역 최소점에 빠지게 된다면 더 이상 손실 값을 낮추지 못하고 낮은 성능의 모델을 얻을 가능성도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41fa801",
   "metadata": {},
   "source": [
    "### 4. 다차원으로 확장\n",
    "\n",
    "다음과 같이 2차원 벡터를 입력으로 받는 함수에 대해서 생각해보자.\n",
    "<img src = \"다차원1.jpg\" width = \"450\" height = \"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36ceba",
   "metadata": {},
   "source": [
    "마치 다음 그림과 같이 2차원 공간의 평면에서 공을 굴리는 것과 같은 일이 될 것이다.  \n",
    "공은 그래디언트 화살표와 반대로 굴러간다.\n",
    "<img src = \"다차원2.jpg\" width = \"400\" height = \"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3f6b5",
   "metadata": {},
   "source": [
    "### 5. 모델 가중치 파라미터로 확장\n",
    "\n",
    "모델 함수가 선형 계층 함수로 구성되어 있다고 가정한다면 가중치 파라미터를 다음과 같이 정의할 수 있다.\n",
    "<img src = \"모델가중치1.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "수식에서 볼 수 있듯이 우리는 n*(m+1) 차원 공간에서 최적의 가중치 파라미터를 찾아야 한다.  \n",
    "이를 위해서 각 가중치 파라미터 W와 b로 손실 함수를 미분하여 그래디언트를 구하고 가중치 파라미터를 업데이트 할 수 있다.\n",
    "<img src = \"모델가중치2.jpg\" width = \"300\" height = \"300\">\n",
    "\n",
    "이것을 파라미터 theta로 묶어서 한 번에 표기하면,\n",
    "<img src = \"모델가중치3.jpg\" width = \"300\" height = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa554c",
   "metadata": {},
   "source": [
    "# 6.4 학습률에 따른 성질\n",
    "\n",
    "앞의 수식에서 n으로 표현된 학습률은 보통 0에서 1 사이의 값(사용자에 의해 주어진)을 가진 상수로서 기울기 벡터인 그래디언트에 곱해진다.  \n",
    "학습률이 0에 가까울수록 파라미터 업데이트의 양은 줄어들 것이고 커질수록 파라미터 업데이트의 양은 늘어난다.  \n",
    "파라미터가 한 번 업데이트되는 것을 한\"스텝\" 움직였다 라고 말한다.  \n",
    "학습률이 클수록 스텝이 크고, 작을수록 스텝이 작다.\n",
    "\n",
    "<img src = \"학습률1.jpg\" width = \"500\" height = \"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43361f6c",
   "metadata": {},
   "source": [
    "왼쪽 그림은 학습률이 클 때를 나타낸 것이다.  \n",
    "학습률이 너무 커서 손실 값이 발산해 버리는 상황을 마주할 수 있는데, 학습률을 50% 또는 10%로 줄여주는 것이 좋다.\n",
    "\n",
    "오른쪽 그림은 학습률이 작을 때를 나타낸 것이다.  \n",
    "손실 표면이 그림과 같이 울퉁불퉁 굴곡진 상황에서는 자칫 지역최소점에 빠질 수 있다. 또한, 공이 굴러가는 중간에 위치한 평평한 표면에서는 기울기가 0에 가까워져 이동이 더뎌지거나 잘 이동하지 않게 되는 현상에 빠질 수 있다.  \n",
    "분명한 것은 너무 작은 학습률을 사용하면 기울기가 적게 반영되어 업데이트 되므로 학습이 매우 더디게 될 것 이라는 점이다.  \n",
    "\n",
    "사용자에 의해 직접 설정되어야 하는 파라미터를 \"하이퍼파라미터\"라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a94a5",
   "metadata": {},
   "source": [
    "# 6.5 경사하강법 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd5312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c4d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표 텐서 생성\n",
    "target = torch.FloatTensor([[.1,.2,.3],\n",
    "                            [.4,.5,.6],\n",
    "                            [.7,.8,.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b8ab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7046, 0.1709, 0.0958],\n",
      "        [0.4440, 0.8070, 0.3388],\n",
      "        [0.1025, 0.3848, 0.5676]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 값을 갖는 텐서 하나 생성, requires_grad 속성이 True\n",
    "x = torch.rand_like(target)\n",
    "# This means the final scalar will be differentiate by x.\n",
    "x.requires_grad = True\n",
    "# You can get gradient of x, after differentiation.\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4667818d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1347, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 텐서 사이의 손실값 계산\n",
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f265e76",
   "metadata": {},
   "source": [
    "while 반복문을 사용하여 두 텐서 값의 차이가 변수 threshold의 값보다 작아질 때 까지 미분 및 경사하강법을 반복 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03de536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss: 8.1493e-02\n",
      "tensor([[0.5703, 0.1773, 0.1412],\n",
      "        [0.4342, 0.7387, 0.3968],\n",
      "        [0.2352, 0.4771, 0.6415]], requires_grad=True)\n",
      "2-th Loss: 4.9298e-02\n",
      "tensor([[0.4658, 0.1824, 0.1765],\n",
      "        [0.4266, 0.6857, 0.4420],\n",
      "        [0.3385, 0.5489, 0.6989]], requires_grad=True)\n",
      "3-th Loss: 2.9822e-02\n",
      "tensor([[0.3845, 0.1863, 0.2039],\n",
      "        [0.4207, 0.6444, 0.4771],\n",
      "        [0.4189, 0.6047, 0.7436]], requires_grad=True)\n",
      "4-th Loss: 1.8041e-02\n",
      "tensor([[0.3213, 0.1893, 0.2253],\n",
      "        [0.4161, 0.6123, 0.5044],\n",
      "        [0.4813, 0.6481, 0.7784]], requires_grad=True)\n",
      "5-th Loss: 1.0913e-02\n",
      "tensor([[0.2721, 0.1917, 0.2419],\n",
      "        [0.4125, 0.5874, 0.5257],\n",
      "        [0.5299, 0.6818, 0.8054]], requires_grad=True)\n",
      "6-th Loss: 6.6020e-03\n",
      "tensor([[0.2339, 0.1936, 0.2548],\n",
      "        [0.4097, 0.5680, 0.5422],\n",
      "        [0.5677, 0.7081, 0.8264]], requires_grad=True)\n",
      "7-th Loss: 3.9938e-03\n",
      "tensor([[0.2041, 0.1950, 0.2648],\n",
      "        [0.4076, 0.5529, 0.5550],\n",
      "        [0.5971, 0.7285, 0.8428]], requires_grad=True)\n",
      "8-th Loss: 2.4160e-03\n",
      "tensor([[0.1810, 0.1961, 0.2727],\n",
      "        [0.4059, 0.5411, 0.5650],\n",
      "        [0.6200, 0.7444, 0.8555]], requires_grad=True)\n",
      "9-th Loss: 1.4615e-03\n",
      "tensor([[0.1630, 0.1970, 0.2787],\n",
      "        [0.4046, 0.5320, 0.5728],\n",
      "        [0.6378, 0.7568, 0.8654]], requires_grad=True)\n",
      "10-th Loss: 8.8413e-04\n",
      "tensor([[0.1490, 0.1976, 0.2835],\n",
      "        [0.4036, 0.5249, 0.5788],\n",
      "        [0.6516, 0.7664, 0.8731]], requires_grad=True)\n",
      "11-th Loss: 5.3485e-04\n",
      "tensor([[0.1381, 0.1982, 0.2871],\n",
      "        [0.4028, 0.5193, 0.5835],\n",
      "        [0.6623, 0.7738, 0.8791]], requires_grad=True)\n",
      "12-th Loss: 3.2355e-04\n",
      "tensor([[0.1296, 0.1986, 0.2900],\n",
      "        [0.4022, 0.5150, 0.5872],\n",
      "        [0.6707, 0.7797, 0.8837]], requires_grad=True)\n",
      "13-th Loss: 1.9573e-04\n",
      "tensor([[0.1230, 0.1989, 0.2922],\n",
      "        [0.4017, 0.5117, 0.5900],\n",
      "        [0.6772, 0.7842, 0.8873]], requires_grad=True)\n",
      "14-th Loss: 1.1840e-04\n",
      "tensor([[0.1179, 0.1991, 0.2939],\n",
      "        [0.4013, 0.5091, 0.5923],\n",
      "        [0.6823, 0.7877, 0.8901]], requires_grad=True)\n",
      "15-th Loss: 7.1626e-05\n",
      "tensor([[0.1139, 0.1993, 0.2953],\n",
      "        [0.4010, 0.5071, 0.5940],\n",
      "        [0.6862, 0.7904, 0.8923]], requires_grad=True)\n",
      "16-th Loss: 4.3330e-05\n",
      "tensor([[0.1108, 0.1995, 0.2963],\n",
      "        [0.4008, 0.5055, 0.5953],\n",
      "        [0.6893, 0.7926, 0.8940]], requires_grad=True)\n",
      "17-th Loss: 2.6212e-05\n",
      "tensor([[0.1084, 0.1996, 0.2972],\n",
      "        [0.4006, 0.5043, 0.5964],\n",
      "        [0.6917, 0.7942, 0.8954]], requires_grad=True)\n",
      "18-th Loss: 1.5856e-05\n",
      "tensor([[0.1066, 0.1997, 0.2978],\n",
      "        [0.4005, 0.5033, 0.5972],\n",
      "        [0.6935, 0.7955, 0.8964]], requires_grad=True)\n",
      "19-th Loss: 9.5922e-06\n",
      "tensor([[0.1051, 0.1998, 0.2983],\n",
      "        [0.4004, 0.5026, 0.5978],\n",
      "        [0.6950, 0.7965, 0.8972]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    loss.backward() # Calculate gradients.\n",
    "    \n",
    "    x = x - learning_rate * x.grad\n",
    "    \n",
    "    # You don't need to aware following two lines, now.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c40c3",
   "metadata": {},
   "source": [
    "backward 함수를 통해 편미분을 수행한다는 것이 중요하다.  \n",
    "편미분을 통해 얻어진 그래디언트들이 x.grad에 자동으로 저장되고 이 값을 활용하여 경사하강법을 수행한다.  \n",
    "참고로 backward를 호출하기 위한 텐서의 크기는 스칼라여야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927a5b3",
   "metadata": {},
   "source": [
    "# 6.6 파이토치 오토그래드 소개\n",
    "\n",
    "오토그래드 : 자동 미분 기능 제공  \n",
    "파이토치는 requires_grad 속성이 True인 텐서의 연산을 추적하기 위한 계산 그래프를 구축하고, backward 함수가 호출되면 이 그래프를 따라 미분을 자동으로 수행하고 계산된 그래디언트를 채워 놓는다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2f5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([[1,2],\n",
    "                       [3,4]]).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec515d",
   "metadata": {},
   "source": [
    "이렇게 requires_grad 속성이 True인 텐서가 있을 때 이 텐서가 들어간 연산의 결과가 담긴 텐서도 자동으로 requires_grad 속성값을 True로 갖게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e9f3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x1 = x + 2\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b68c6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x2 = x - 2\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b21b800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x3 = x1 * x2\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0355c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x3.sum()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce4124",
   "metadata": {},
   "source": [
    "코드 실행으로 생성된 결과 텐서들이 모두 grad_fn 속성을 갖는다.  \n",
    "예를들어 x1의 덧셈 연산의 결과물이기 때문에 x1의 grad_fn 속성은 AddBackward0임을 볼 수 있다.  \n",
    "텐서 y는 sum 함수를 썼으므로 스칼라 값이 되었다. 여기서 다음과 같이 backward 함수를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fe59efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d56a5d",
   "metadata": {},
   "source": [
    "그러면 x,x1,x2,x3,y 모두 grad속성에 그래디언트 값이 계산되어 저장되었을 것이다.  \n",
    "이것을 수식으로 나타내면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94737a85",
   "metadata": {},
   "source": [
    "<img src = \"오토그래드1.jpg\" width = \"400\" height = \"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04acb27a",
   "metadata": {},
   "source": [
    "y를 다시 x로 미분하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b88ee",
   "metadata": {},
   "source": [
    "<img src = \"오토그래드2.jpg\" width = \"300\" height = \"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6939349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4126072",
   "metadata": {},
   "source": [
    "이 연산에 사용된 텐서들을 배열과 같은 곳에 저장할 경우 메모리 누수의 원인이 되기 때문에 주의해야 한다.  \n",
    "만약에 x3을 배열 등에 저장해두지 않을 경우 x3뿐만 아니라 계산 그래프에 등록된 텐서들 모두 메모리에서 해제되지 않는다.  \n",
    "따라서 detach함수를 통해 그래프로부터 떼어낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7e145f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.,  0.],\n",
       "        [ 5., 12.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
