{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17.2 기본 확률 통계\n",
    "\n",
    "변수들은 연속 값을 갖는 변수일 수도 있고, 이산 값을 갖는 변수일 수도 있다. 이 둘의 가장 중요한 차이는 이산 확률 변수가 특정 값을 가질 확률은 구할 수 있지마\\만, 연속 확률 변수가 특정 값을 가질 확률은 0이다.  \n",
    "이산 확률 변수가 특정 값을 가질 확률을 확률 질량이라고 한다. 연속 확률 변수의 경우 특정 값에 대한 확률은 확률 밀도를 통해 나타낼 수 있다.  \n",
    "확률 밀도를 구간에 대해서 적분하면 우리가 흔히 말하는 확률 값이 된다.  \n",
    "\n",
    "즉, 이산 확률 변수는 합을 통해 확률을 구할 수 있지만, 연속 확률 변수의 경우에는 적분을 통해 구간에 대한 확률을 구할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 결합 확률\n",
    "\n",
    "변수가 2개 이상인 확률 분포에 대해서도 수식으로 표현할 수 있다.\n",
    "\n",
    "<img src = \"확률론1.jpg\" width = \"400\" height = \"100\">\n",
    "\n",
    "이와 같이 두 개 이상의 변수를 사용하는 확률 표현을 결합 확률이라고 한다.\n",
    "\n",
    "<img src = \"확률론2.jpg\" width = \"400\" height = \"300\">\n",
    "\n",
    "만약에 두 변수가 독립적이라면 각 변수의 확률 곱으로도 나타낼 수 있다.  \n",
    "머신러닝이나 딥러닝 문제에서는 입력과 출력 등을 확률 변수로 나타낼 수 있는데 일반적으로 입력샘플을 나타내는 변수를 x, 출력 샘플을 y로 사용하여 결합 확률로 나타내는 경우가 많다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 조건부 확률\n",
    "\n",
    "조건부 확률과 결합 확률과의 관계는 다음과 같다.\n",
    "\n",
    "<img src = \"확률론3.jpg\" width = \"400\" height = \"300\">\n",
    "\n",
    "이러한 조건부 확률과 결합 확률의 관계를 활용한것이 베이즈 정리 이다.   \n",
    "베이즈 정리를 활용하면 조건부 확률의 앞과 뒤에 나타나는 변수 위치를 반대로 바꿀 수 있다. 베이즈 정리 또한 머신러닝에서 굉장히 중요한 역할을 한다.\n",
    "\n",
    "P(h|D) = P(D|h)P(h) / P(D)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수?값?분포?함수?\n",
    "\n",
    "앞의 수식들을 살펴보면 P()안에 사용되는 글꼴이 다르다는 것을 알 수 있다. 예를들어 확률변수를 나타낼 때는 x로 나타내고 해당 변수가 어떤 값을 가지는경우는 필기체 x로 나타낸다.\n",
    "\n",
    "<img src = \"확률론4.jpg\" width = \"400\" height = \"100\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"확률론5.jpg\" width = \"700\" height = \"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 형태는 변수 x에 어떤 값이 주어졌을 때 변수 y가 y값을 가질 확률 값을 나타낸다. 만약 조건 변수의 값이 바뀐다면 확률 값도 바뀔 것이다. 따라서 이것을 확률의 형태로 바꾼다면 조건부 변수를 함수의 입력, 확률 값을 함수의 출력으로 생각할 수 있다.\n",
    "\n",
    "P(x|y)와 같은 형태의 함수 꼴도 생각해보자면, 입력의 형태는 같지만 출력 형태가 확률값이 아닌 확률 분포가 될 것이다.  \n",
    "함수 출력이 확률 분포라는 것을 파이썬에 적용해보자면 함수가 실수형 값을 반환하는 것이 아니라 분포라는 객체를 반환하는 것으로 생각할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 주변 분포\n",
    "\n",
    "결합 확률 분포가 있을 때 하나의 변수를 적분해서 없앨 수도 있다. 이것을 주변 분포라고 한다.\n",
    "\n",
    "<img src = \"확률론6.jpg\" width = \"400\" height = \"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17.3 MLE(Maximum Likelihood Estimation)\n",
    "\n",
    "앞에서 확률 분포에 대해 언급한 이유는 딥러닝 또한 확률 분포와 밀접한 연관이 있기 때문이다.  \n",
    "딥러닝이 학습되는 원리인 최대 가능도 방법(MLE)에 대해서 알아본다.\n",
    "\n",
    "샘플을 충분히 모은 후 분포가 정규 분포를 따른다는 가정 하에 정규 분포의 평균과 표준편차를 계산할 수 있다. 이처럼 확률 분포의 형태를 정의하는 값을 분포의 파라미터라고 부른다.  \n",
    "만약 만들어진 정규 분포가 적절하다면 주어진 샘플들이 만들어진 분포 위에서 높은 확률을 지녀야 한다. 따라서 샘플 위의 점선들의 길이가 최대가 되었으면 한다.\n",
    "\n",
    "<img src = \"확률론7.jpg\" width = \"400\" height = \"300\">\n",
    "\n",
    "이 점선의 길이의 곱을 가능도 라고 한다. 결과적으로 우리는 이 점선 길이들의 곱인 가능도를 최대로 하는 분포의 파라미터를 찾아내려던 것이고, 이 과정을 최대 가능도 방법이라 부른다.\n",
    "\n",
    "데이터셋 D가 주어졌을 때 정규 분포의 파라미터에 대한 함수로 표현할 수 있다. 파라미터의 변화에 따라서 가능도의 크기가 바뀔 것이다.\n",
    "즉, 가능도는 현재 분포의 파라미터가 수집된 데이터를 얼마나 잘 설명하는지 나타내는 점수라고 볼 수 있다.  \n",
    "가능도 함수는 분포의 파라미터의 변화에 따라 변화하는 가능도를 나타낸 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 로그 가능도\n",
    "\n",
    "가능도는 확률의 곱으로 표현 된다. 따라서 샘플의 숫자가 많아지면 가능도의 크기는 굉장히 작아질 가능성이 높다. 이것을 계산할 때 언더플로에 빠질 가능성이 넢기 때문에 로그를 도입하여 확률의 곱셈을 덧셈으로 만들어준다.\n",
    "\n",
    "<img src = \"확률론8.jpg\" width = \"400\" height = \"100\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 경사상승법을 통한 MLE\n",
    "\n",
    "앞에서 MLE를 수행할 때 임의의 파라미터를 생성해서 비교하는 작업을 반복했다. 대부분 최적의 파라미터를 찾는데 굉장히 오랜 시간이 걸릴 것이다.   \n",
    "이때 딥러닝에서 사용하던 경사하강법과 같은 경사상승법을 활용하여 MLE를 수행할 수 있다.\n",
    "\n",
    "<img src = \"확률론9.jpg\" width = \"400\" height = \"100\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17.4 신경망과 MLE\n",
    "\n",
    "가능도는 데이터 샘플을 주어진 분포의 파라미터로 얼마나 잘 설명하는지 수치화한 것 이다.\n",
    "\n",
    "<img src = \"확률론10.jpg\" width = \"400\" height = \"150\">\n",
    "\n",
    "이 수식에서 logP(yi|xi)는 theta라는 파라미터를 갖는 분포에 xi가 주어졌을 때, yi의 확률을 의미하며 하나의 샘플 쌍에 대한 가능도가 된다.  \n",
    "또한 분포의 파라미터 표현과 관련해서 다음 수식은 모두 같은 표현이다.\n",
    "\n",
    "<img src = \"확률론11.jpg\" width = \"400\" height = \"150\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 신경망의 출력과 확률 분포\n",
    "\n",
    "심층신경망의 가중치 파라미터가 분포를 나타내는 파라미터이며, 신경망의 출력이 가중치 파라미터의 변화에 따른 확률 분포라고 볼 수 있다.  \n",
    "다음과 같은 심층신경망이 주어졌을 때 3개의 계층의 각 가중치 파라미터들은 신경망의 동작을 정의하게 되고 결과적으로 분포의 파라미터로 취급받을 수 있다.\n",
    "\n",
    "<img src = \"확률론12.jpg\" width = \"400\" height = \"300\">\n",
    "\n",
    "그러면 앞서 언급했던 MLE 방식과 마찬가지로 최대 가능도를 만드는 방향으로 가중치 파라미터 theta를 경사상승법을 통해 업데이트할 수 있게 된다.  \n",
    "하지만 파이토치는 경사하강법만 지원하므로 NLL(negative log likehood)을 도입하여 최대화 문제를 최소화 문제로 바꾸고 경사하강법을 통해 MLE를 구현할 수 있다.  \n",
    "\n",
    "<img src = \"확률론13.jpg\" width = \"400\" height = \"300\">\n",
    "\n",
    "이전 방식에서는 y^i = f(xi)라고 하고 손실함수를 정의 했는데 MLE 방식을 활용한 NLL목적 함수에서는 신경망이 어떻게 활용되는 것 일까?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17.5 수식: MLE\n",
    "\n",
    "<img src = \"확률론14.jpg\" width = \"400\" height = \"150\">\n",
    "\n",
    "소프트맥스 계층으로부터 출력된 벡터 y^ 의 각 차원들은 미리 지정된 클래스에 대한 확률 값을 담고 있을 것이다.  \n",
    "우리가 찾고자 하는 파라미터는 다음과 같이 NLL함수를 최소화 하는 파라미터가 될 것이다.\n",
    "\n",
    "<img src = \"확률론15.jpg\" width = \"400\" height = \"150\">\n",
    "\n",
    "여기서 심층 신경망의 출력 벡터 y^i = f(xi)는 소프트맥스 함수의 출력값이므로 벡터의 각 차원은 클래스에 대한 확률 값을 담고 있는데 이것은 이산 확률 분포로 생각할 수 있다.  \n",
    "그럼 로그 가능도 logP(yi|xi theta)는 다음과 같이 구할 수 있다.\n",
    "\n",
    "<img src = \"확률론16.jpg\" width = \"400\" height = \"150\">\n",
    "\n",
    "이것을 실제 벡터 수준에서 예제를 통해 살펴보면 다음과 같이 진행될 것 이다. 예를 들어 다음과 같이 정답 벡터 yi와 출력 벡터 y^i가 있다고 할 때\n",
    "\n",
    "<img src = \"확률론17.jpg\" width = \"400\" height = \"150\">  \n",
    "\n",
    "앞의 수식에 따라 내적을 취하면 다음과 같이 로그 가능도를 계산할 수 있다.  \n",
    "\n",
    "<img src = \"확률론18.jpg\" width = \"400\" height = \"150\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 교차 엔트로피와 NLL\n",
    "\n",
    "앞에서 분류문제에 대해서 배울 때 교차 엔트로피에 대해서 설명했다. 교차 엔트로피의 수식은 다음과 같다.  \n",
    "\n",
    "<img src = \"확률론19.jpg\" width = \"400\" height = \"150\">\n",
    "\n",
    "교차 엔트로피를 통해 심층신경망을 학습하기 위해 파라미터 theta에 대한 손실 함수로 구성하면 다음과 같을 것이다.  \n",
    "\n",
    "<img src = \"확률론20.jpg\" width = \"400\" height = \"150\">\n",
    "\n",
    "교차 엔트로피 = 손실 함수와 NLL 손실 함수의 모습이 닮아 있는 것을 확인할 수 있다.  \n",
    "\n",
    "교차 엔트로피 손실 함수를 통해 심층신경망을 학습한는 것은 NLL 손실 함수 및 MLE를 통해 심층신경망을 학습하는 것과 같다.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17.6 MSE 손실함수와 MLE\n",
    "\n",
    "회귀 문제에서는 MSE 손실 함수를 통해 심층신경망을 학습시키키 때문에 MLE를 비롯한 로그 가능도는 분류 문제에만 해당되는 것은 아닐까 라는 의문이 생길수도 있지만, MSE 손실 함수의 경우에도 동작하고 있다.  \n",
    "즉, 회귀 문제에서 신경망이 가우시안 분포의 평균을 출력하고 있디면, 분류 문제와 같은 원리 내에서 동작하고 있음을 알 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약\n",
    "\n",
    "### 가능도  \n",
    "* 가능도란 데이터 샘플들을 주어진 분포의 파라미터로 얼마나 잘 설명하는지 수치화한 것\n",
    "\n",
    "### MLE\n",
    "* 심층신경망을 확률 분포함수로 해석할 수 있으며, 이에 따라 MLE를 통해 모델을 학습할 수 있음\n",
    "* 음의 가능도 손실 함수를 통해 MLE를 수행\n",
    "* NLL 손실 함수는 교차 엔트로피 손실 함수와 수식이 거의 같음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
