{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.1 순환신경망 소개"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"rnn1.jpg\" width = \"400\" height = \"200\">  \n",
    "\n",
    "위 그림에서 x(t) 또는 h(t)라는 표현을 보면 t가 추가되어 순서를 표기하고 있음을 알 수 있다.  \n",
    "그리고 그 t가 끝에 다다르면 비로소 y=h(t)가 되어 출력값을 얻게 된다.  \n",
    "h(t)를 얻기 위해서 x(t)뿐만 아니라 h(t-1)도 함께 함수의 입력으로 주어져야 한다는 것 이다.  \n",
    "즉, 이전 순서에서의 상태 결과인 h(t-1)을 현재 입력 x(t)와 함께 함수에 넣어주어 현재 상태 결과인 h(t)를 구하도록 되어있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 신경망의 형태를 순환신경망(RNN)이라고 부른다.  \n",
    "기존 신경망은 주로 테이블 데이터나 이미지 데이터를 다루는 데 사용되었지만 RNN은 자연어 처리와 같이 순서 정보가 담긴 데이터나 시계열 데이터를 다루는 데 적합하다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.2 RNN 한 걸음씩 들여다보기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 기본적인 RNN의 구조"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 다음의 수식과 같이 네개의 가중치 파라미터를 갖는다.\n",
    "\n",
    "<img src = \"rnn1.jpg\" width = \"400\" height = \"200\">\n",
    "\n",
    "w(ih),b(ih)는 입력 x(t)에 곱해지고 더해지는 파라미터가 되고, w(hh),b(hh)는 이전 순서의 결과값인 h(t-1)에 곱해지고 더해지는 파라미터가 된다.\n",
    "\n",
    "이 연산 결괏값에 하이퍼볼릭 탄젠트를 통과시켜 현재 순서의 h(t)를 얻을 수 있다.  \n",
    "이 h(t)를 RNN의 은닉 상태라고 부른다.  \n",
    "\n",
    "<img src = \"rnn3.jpg\" width = \"400\" height = \"100\">  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN을 학습시키는 방법은 여러가지지만 모든 순서의 결괏값(은닉 상태)h(t)들을 출력으로 취급하여 학습하는 것이다.  \n",
    "정답 또한 순서 데이터로 y={y(1)...y(T)}와 같이 갖고 있어야 한다.  \n",
    "다음 그림과 같이 h(t)->y^(t)가 되어 실제 정답과 비교하는 손실 함수를 구성할 수 있다.  \n",
    "\n",
    "<img src = \"rnn4.jpg\" width = \"400\" height = \"200\">  \n",
    "\n",
    "이러한 형태의 RNN을 활용하면 가변 길이의 순서데이터를 다룰 수 있다.  \n",
    "순서 데이터는 각 순서에 나타나는 값에 따라 앞뒤 순서의 값이 영향을 받을 뿐만 아니라 전체 순서 데이터의 의미가 결정되기도 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RNN의 입출력 텐서 형태"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 RNN의 입력 텐서 모양을 시각화 한 것 이다.\n",
    "\n",
    "<img src = \"rnn5.jpg\" width = \"400\" height = \"200\">  \n",
    "\n",
    "하나의 순서에 대한 텐서를 x(t)라고 할 때 앞 그림의 왼쪽과 같은 텐서가 존재할 것이고, 순서가 n개 있다면 오른쪽 그림과 같은 텐서가 될 것이다.  \n",
    "텐서의 첫 번째 차원은 미니배치 내의 인덱스를 기리키고, 두 번째 차원은 순서 정보를 가지며 마지막 차원은 입력 벡터가 된다. \n",
    "다음 그림은 RNN의 입력이 들어간 후 반환되는 출력 텐서이다.  \n",
    "\n",
    "<img src = \"rnn6.jpg\" width = \"400\" height = \"200\">  \n",
    "\n",
    "RNN의 출력 텐서 형태에서는 순서 정보에 대한 차원이 빠지지만, (batch_size,hidden_size) = (batch_size,1,hidden_size)이므로 여전히 같은 텐서의 형태라고 봐도 무방하다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다계층 순환신경망"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 계층이나 합성곱 계층을 여러층 쌓아서 심층신경망을 만들었던 것처럼 RNN도 여러층을 쌓아서 깊게 만들 수 있다.  \n",
    "이것을 다계층 순환신경망이라고 부른다.  \n",
    "다계층 순환신경망도 각 순서마다 y^(t)를 반환하며, 실제 정답 y(t)와 비교해서 손실 함수를 계산하는 것을 볼 수 있다.  \n",
    "\n",
    "단일 계층 순환신경망에서 오른쪽으로 빠지는 화살표를 은닉 상태h(t)라고 했었고, 이것이 곧 출력 y^(t)라고 했다.  \n",
    "하지만 다계층 순환신경망에서는 여러 층이 동시에 h(t)를 반환하기 때문에 y^(t)와 h(t)가 같을 수 없다.\n",
    "\n",
    "<img src = \"rnn7.jpg\" width = \"400\" height = \"300\">  \n",
    "\n",
    "수식을 살펴보면 각 계층마다 h(t,l)을 반환하고, 마지막 계층의 h(t,l)을 받아서 y^(t)로 삼고 있음을 볼 수 있다.  \n",
    "즉, 다계층 순환신경망에서 은닉 상태는 모델의 출력이 되지 않는다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 그림에서 빨간색 점선 네모로 표시된 부분이 다계층 순환신경망의 입력이 되는 부분이다.\n",
    "\n",
    "<img src = \"rnn8.jpg\" width = \"400\" height = \"200\">  \n",
    "\n",
    "입력 형태는 단일 계층 순환신경망의 입력 텐서의 형태와 같다.  \n",
    "다음은 출력 텐서이다.  \n",
    "\n",
    "<img src = \"rnn9.jpg\" width = \"400\" height = \"200\">  \n",
    "\n",
    "마찬가지로 단일 계층 순환신경망의 출력 텐서와 형태가 같다.  \n",
    "하지만 단일 계층 순환신경망에서는 은닉 상태가 곧 출력이었지만 다계층 순환신경망에서는 출력과 은닉 상태가 다르다.  \n",
    "다음은 다계층 순환신경망의 은닉 상태를 빨간색 점선 네모로 표시한 그림이다.  \n",
    "\n",
    "<img src = \"rnn10.jpg\" width = \"400\" height = \"200\">  \n",
    "\n",
    "위 그림에서 볼 수 있듯이 빨간색 점선 네모의 위치가 달라졌고, 텐서의 모양도 다르다.  \n",
    "가장 주목할 부분은 앞의 출력 텐서는 순서 정보가 두 번째 차원에 들어가있는 반면에 은닉 상태 텐서는 어떤 특정 순서 상에서 얻을 것이기 때문에 순서 정보가 텐서에 없다.  \n",
    "대신 가장 첫 번째 차원에 미니배치와 관련된 정보가 아니라 계층 순서에 대한 정보가 담겨있다.  \n",
    "그리고 두 번째 차원에 미니배치에 대한 정보가 담겨있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 은닉 상태"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉 상태에는 순환신경망이 현재 순서까지 입력 x1 ... x(t)들을 받아오면서 자신의 상태를 업데이트한 기억을 갖고 있다고 볼 수 있다.  \n",
    "은닉 상태는 신경망을 통과하는 값일 뿐이며 학습하는 가중치 파라미터가 아니다.  \n",
    "사람과 비유하면 우리가 어떤 입력을 받아 생각하는 것은 가중치 파라미터와 연산을 하는 것이라고 볼 수 있다.  \n",
    "행동은 모델의 출력 y^(t)라고 볼 수 있고, 우리가 기억하는 것이 모델의 은닉 상태  h(t)라고 볼 수 있을 것이다.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 양방향 다계층 순환신경망"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 RNN은 한 방향으로만 은닉 상태가 흐르는 모델이었다.  즉, 현재 순서의 은닉 상태는 이전 순서의 은닉 상태와 현재 입력에만 의존한다.  \n",
    "이런 모델을 자기회귀 모델이라고 부른다. 하지만 이번에는 역방향이 추가된 양방향 순환신경망을 살펴본다.  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"rnn12.jpg\" width = \"400\" height = \"200\">  \n",
    "\n",
    "양방향 다계층 순환신경망의 각 층의 RNN셀은 이전 계층의 정방향과 역뱡향 결과물을 입력으로 받는다. 그리고 정방향 RNN셀은 이전 순서의 은닉 상태를 받고 RNN셀은 미래 순서의 은닉 상태를 받는다.\n",
    "\n",
    "모델의 입력 텐서 모양은 앞에서 살펴보았던 모델들의 입력 텐서 모양과 똑같다. 그리고 출력 텐서 모양은 다음과 같다.\n",
    "\n",
    "<img src = \"rnn13.jpg\" width = \"400\" height = \"200\">  \n",
    "\n",
    "위 그림의 빨간색 점선으로 표시된 부분에서 볼 수 있듯이 출력 텐서는 마지막 계층의 정방향과 역방향 RNN 셀로부터 출력을 수집한다.   \n",
    "따라서 텐서 형태의 마지막 차원이 hidden_size x #drection로 되어 기존의 두배가 된 것을 볼 수 있다.\n",
    "\n",
    "보통 단방향 순환신경망은 은닉 상태를 따로 저장했다가 이어 연산을 진행하는 등의 작업을 수행하는 경우도 많아 은닉 상태를 직접 다룰 일이 많지만  \n",
    "양방향 순환신경망은 중간에 은닉 상태를 접근할 일이 거의 없다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.3 순환신경망 활용 사례\n",
    "\n",
    "<img src = \"rnn14.jpg\" width = \"400\" height = \"250\">  \n",
    "\n",
    "RNN이 가장 많이 활용되는 분야는 단연 자연아 처리이다. 자연어 처리에서 문장은 출현 단어 개수가 가변적이며 단어의 출현 순서에 따라 의미가 결정된다.  \n",
    "이처럼 입력과 출력의 종류에 따라 활용 타입을 정의할 수 있지만 모델링하고자 하는 대상의 자기회귀 성격 여부에 따라 활용 방법이 달라지기도 한다.  \n",
    "자기회귀란 현재 상태가 과거 상태에 의존하여 정해지는 경우를 말한다.  \n",
    "따라서 정보 흐름의 방향이 생기게 되고 이것을 모델링 하기 위해서는 단방향 순환신경망을 사용할 수 밖에 없다.  \n",
    "문장 전체를 놓고 문장이 속하는 클래스(긍정 or 부정)를 정하는 문제는 이미 분류기에 들어가기 전에 문장 전체가 주어진다.  \n",
    "또한 클래스는 문장 전체에 대해서 단 한번 예측이 수행되기 때문에 자기회기 성격을 가진다고 볼 수 없다.\n",
    "\n",
    "<img src = \"rnn15.jpg\" width = \"400\" height = \"150\"> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 다대일 형태"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다대일 형태는 우리가 학습할 데이터에서 입력은 순서 정보를 갖고 있고, 출력은 순서 정보가 없는 경우이다.  \n",
    "이 케이스는 비자기회귀 성격을 갖는다고 볼 수 있다.  \n",
    "\n",
    "<img src = \"rnn16.jpg\" width = \"400\" height = \"300\"> \n",
    "\n",
    "앞의 그림에서 볼 수 있듯이 양방향 순환신경망을 사용할 수 있고, 보통 마지막 순서 출력을 활용하여(처음 순서 출력을 이용해도 아랑곳 x) 모델의 예측 값 y^으로 삼고 원래 목푯값인 y와 비교하여 손실 값을 계산한다.  \n",
    "텍스트 분류가 가장 좋은 예제라고 볼 수 있다. 텍스트 문장을 정해진 기준에 따라 쪼개어 순환신경망의 입력으로 삼아 각 순서에 맞게 나누어 넣어준다.  \n",
    "그럼 마지막 계층의 순서에서 나온 출력을 비선현 활성 함수, 선형 계층, 그리고 소프트맥스 함수를 거치도록 하여 각 클래스별 확률 값으로 변환할 수 있다.  \n",
    "그리고 분류 문제이기 때문에 교차 엔트로피 손실 함수를 통과시켜 손실 값을 구할 수 있을 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 다대다 형태"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다대다 형태는 입력과 출력의 순서 개수가 같아야 하며 각 순서가 입력에 대응되는 형태여야 한다.  \n",
    "만약 순서 개수가 다르거나 대응이 다르게 되면 일대다 형태가 되어야 한다.  \n",
    "다대다 형태도 비자기회귀 성격을 갖는다.\n",
    "\n",
    "<img src = \"rnn17.jpg\" width = \"400\" height = \"300\"> \n",
    "\n",
    "각 순서의 입력 x(t)마다 대응되는 목푯값 y(t)가 존재하는 것을 볼 수 있고, 모델에서도 각 입력에 대응되는 y^(t)가출력되는 것을 볼 수 있다.  \n",
    "그러면 모든 순서의 출력값과 목푯값의 차이를 합쳐서 손실값을 계산할 수 있다.  \n",
    "\n",
    "이러한 대다대 형태의 가장 흔한 예제로 형태소 분석이 있다. 문장 내 토큰들에 대해서 각각 형태소가 태깅되어야 하기 때문에 입력 순서들에 1:1로 출력값이 존재해야 하기 때문이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 일대다 형태"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일대다 형태는 생성모델에 많이 활용된다.  \n",
    "일대다는 입력의 형태보다는 출력의 형태가 중요하다. 입력 순서에  1:1 대응이 되는것이 아니라 출력에 순서 정보가 존재하는 데이터라면 대부분 일대다 문제가 적용될 수 있다. 가장 흔한 예제로는 자연어 생성 문제인 기계 번역이나 챗봇 등이 있다.  \n",
    "또한 이미지가 주어졌을 때 이미지를 설명하는 문장을 생성하는 문제도 여기에 해당된다. \n",
    "\n",
    "<img src = \"rnn18.jpg\" width = \"400\" height = \"300\"> \n",
    "\n",
    "일단 모델의 입력이 x 하나로 되어있고 이후 순서에는 이전 수서의 모델 출력값 y^(t-1)이 넣어지는 것을 확인할 수 있다.  \n",
    "따라서 y0=x 라고 가정한다면 y^(t)는 y^(t-1)의 영향을 받아서 자기회귀 성격을 갖게 되는 것을 볼 수 있다.  \n",
    "\n",
    "만약 입력 x에 이미지에 대한 정보를 넣어준다면 이미지에 대한 설명 문장을 생성할 수 있을 것이고, 한글 문장에 대한 정보가 들어있다고 한다면 대응되는 영어 번역 문장을 생성할 수 있을 것이다.  \n",
    "또는 질문 문장에 대한 정보가 들어있다면 이에 대한 대답 문장을 생성할 수 있을 것이다.\n",
    "\n",
    "이런 문제를 해결하기 위해서는 다대일 형태와 일대다 형태를 결합한 시퀀스투시퀀스를 활용할 수 있다. \n",
    "다음은 Seq2Seq를 도식화 한 것이다.\n",
    "\n",
    "<img src = \"rnn19.jpg\" width = \"400\" height = \"200\"> \n",
    "\n",
    "그림의 왼쪽 부분은 다대일 형태의 인코더가 되어 문장을 입력으로 받아서 하나의 벡터형태로 정보를 압축한다면, 다대일 형태의 디코더는 정보를 받아서 순서 데이터인 문장으로 생성해 내는 역할을 수행한다.  \n",
    "이처럼 각 형태들은 자체가 전체 모델로 활용되기도 하지만 전체 모델을 구성하기 위한 서브 모듈로 활용되기도 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.4 LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 공부한 순환신경망은 순서 데이터를 다룰 수 있다는 특징이 있지만 내부에 하이퍼볼릭 탄젠트가 존재하여 그래디언트 소실이 발생한다는 치명적인 단점이 있다.  \n",
    "따라서 긴 순서 데이터를 다룰 경우에 학습 과정에서 그래디언트 소실로 인하여 앞의 순서 데이터에 대해서 파라미터 업데이트가 부실해지게 된다.  \n",
    "결과적으로 기본 RNN을 활용해서 다룰 수 있는 순서 데이터는 짧은 길이로 한정된다. LSTM은 이러한 단점을 보완하는 구조이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 게이트"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수는 전 영역에서 0과 1 사이의 값을 반환한다. 따라서 특정 값에 시그모이드를 곱하면 마치 문을 열고 닫는 듯한 효과를 낼 수 있다.  \n",
    "예를 들어 다음과 같은 수식을 생각해보자. 여기서 x는 벡터의 요소별(element-wise) 곱셈을 표현한다.\n",
    "\n",
    "<img src = \"rnn20.jpg\" width = \"400\" height = \"100\"> \n",
    "\n",
    "이 수식을 살펴보면 x에 시그모이드 함수 결괏값을 곱해주어 값을 취해주는 것을 볼 수 있다.  \n",
    "만약 시그모이드 결과값의 특정 차원이 0에 가깝다면 x의 해당 차원은 거의 가져올 수 없을 것이고 시그모이드 결과값의 특정 차원이 1에 가깝다면 x의 해당 차원은 대부분 가져올 수 있을 것이다.  \n",
    "또한 시그모이드의 입력값은 선형 계층 수식으로 되어있는 것을 볼 수 있다.  \n",
    "x의 값에 따라 선형 계층이 큰 값 또는 작은 값을 반환할 것이고, 이것이 시그모이드를 열고 닫는 효과를 낸다. 이렇게 동작하는 방식을 게이트라고 한다.   \n",
    "이 수식에 따라 학습된 게이트는 현재 데이터 값을 두고 데이터를 통과시킬지 말지 결정하고 행동하게 된다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSTM 수식\n",
    "\n",
    "LSTM은 Long Short Term Memory의 약자로 장단기 기억을 수행할 수 있는 구조 모델이다.  \n",
    "다음은 LSTM으르 도식화 한것인데, 앞서 설명했던 게이트가 여러 개 존재하는 것을 확인할 수 있다.\n",
    "\n",
    "<img src = \"rnn21.jpg\" width = \"400\" height = \"300\"> \n",
    "\n",
    "앞의 그림에는 forget게이트, output게이트, input 게이트 등 3개가 존재하는 것을 볼 수 있다.\n",
    "\n",
    "<img src = \"rnn22.jpg\" width = \"400\" height = \"300\"> \n",
    "\n",
    "눈여겨보아야 할 점은 은닉 상태 h(t) 이외에도 셀 상태라는 개념이 추가 되었다는 것이다.  \n",
    "\n",
    "앞의 그림에서 왼쪽에서 들어오는 이전 순서의 결과값은 h(t-1)과 c(t-1)등 두 개가 되고 마찬가지로 이번 순서의 출력값도 h(t), c(t)가 된다.\n",
    "\n",
    "LSTM은 RNN의 단점인 그래디언트 소실 문제를 해결했지만 구조가 매우 복잡하고 RNN에 비해서 훨씬 더 많은 파라미터를 갖는다는 단점도 존재한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GRU\n",
    "\n",
    "LSTM의 복잡한 구조와 많은 파라미터들의 단점을 보완하고자 GRU가 제안됐다. \n",
    "\n",
    "<img src = \"rnn23.jpg\" width = \"400\" height = \"300\"> \n",
    "\n",
    "LSTM에 비해서 훨씬 간결한 구조를 가지고 있지만 여전히 2개의 게이트를 가지고 있고, 이 게이트가 그래디언트 소실을 방지하는데 매우 중요한 역할을 한다.  \n",
    "LSTM에는 셀 상태 c(t)라는 개념이 추가되었던 것에 비해, GRU는 여전히 은닉 상태 h(t) 하나만 가지고 있어서 실제 구현할 때에도 매우 용이하다.  \n",
    "결과적으로 GRU는 LSTM에 비해 성능이 떨어지지도 않고 더 적은 파라미터를 가지고 그래디언트 소실 문제를 해결한다.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LSTM의 입출력 텐서 형태"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM의 입출력 및 은닉 상태 텐서 모양도 기존 기본 RNN과 같다.\n",
    "\n",
    "<img src = \"rnn24.jpg\" width = \"400\" height = \"100\"> \n",
    "\n",
    "셀 상태 텐서의 모양도 은닉 상태 텐서의 모양과 같음을 주목해야 한다.  \n",
    "그리고 앞에서와 마찬가지로 양방향 LSTM이 될 경우, #direction = 2일 것이다.  \n",
    "GRU도 셀 상태 덴서가 없을 뿐 LSTM과 똑같다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19.5 그래디언트 클리핑"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순환신경망은 BPTT(Back-Propagation Through Time) 알고리즘에 의해서 학습된다.  \n",
    "이때 BPTT 알고리즘에 따라 긴 데이터를 다루는 상황일수록 그래디언트가 더해지는 횟수가 늘어나게 된다.  \n",
    "따라서 데이터의 길이에 따라 적절한 학습률의 크기가 바뀔 수도 있고 잘못하면 그래디언트가 너무 커져서 자칫 학습이 산으로 갈 수도 있다. \n",
    "\n",
    "이러한 상황을 방지하기 위해서 우리는 그래디언트 노름의 최댓값을 정하고 최댓값을 넘길 경우 그래디언트를 강제로 줄여버리도록 할 수 있다.  \n",
    "이 방법을 그래디언트 클리핑이라고 한다.  \n",
    "특징으로는 그래디언트 벡터의 크기는 줄이되 방향은 유지하도록 클리핑 작업을 수행하는 것이다.  \n",
    "그럼 클리핑 된 이후의 그래디언트 방향은 같으므로 파라미터가 업데이트 되는 방향도 같을 것이다.\n",
    "\n",
    "<img src = \"rnn25.jpg\" width = \"400\" height = \"150\"> \n",
    "\n",
    "기존의 경사하강법에 의한 파라미터 업데이트는 그래디언트 클리핑이 더해져서 다음과 같이 바뀐다.\n",
    "\n",
    "<img src = \"rnn26.jpg\" width = \"400\" height = \"200\"> \n",
    "\n",
    "아담(Adam)을 활용하는 경우에도 그래디언트가 너무 커지는 것을 방지하도록 유용하게 쓸 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
